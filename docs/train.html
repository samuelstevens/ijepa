<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.1">
<title>train API documentation</title>
<meta name="description" content="TODO: …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>train</code></h1>
</header>
<section id="section-intro">
<p>TODO:</p>
<ol>
<li>Weight decay</li>
<li>Distributed data parallel</li>
<li>Checkpointing</li>
<li>Shuffle data</li>
</ol>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="train.filter_no_caption_or_no_image"><code class="name flex">
<span>def <span class="ident">filter_no_caption_or_no_image</span></span>(<span>sample)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="train.log_and_continue"><code class="name flex">
<span>def <span class="ident">log_and_continue</span></span>(<span>exn)</span>
</code></dt>
<dd>
<div class="desc"><p>Call in an exception handler to ignore any exception, issue a warning, and continue.</p></div>
</dd>
<dt id="train.make_dataloader"><code class="name flex">
<span>def <span class="ident">make_dataloader</span></span>(<span>args: <a title="train.Args" href="#train.Args">Args</a>, img_transform) ‑> torch.utils.data.dataloader.DataLoader</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="train.make_img_transform"><code class="name flex">
<span>def <span class="ident">make_img_transform</span></span>(<span>resize_size: int, crop_size: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Make the image transform.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resize_size</code></strong></dt>
<dd>How big to resize images to.</dd>
<dt><strong><code>crop_size</code></strong></dt>
<dd>After resizing, how big should the crop be resized to.</dd>
</dl></div>
</dd>
<dt id="train.print_env"><code class="name flex">
<span>def <span class="ident">print_env</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="train.to_json_value"><code class="name flex">
<span>def <span class="ident">to_json_value</span></span>(<span>value: object)</span>
</code></dt>
<dd>
<div class="desc"><p>Recursively converts objects into JSON-compatible values.</p>
<p>As a fallback, tries to call <code><a title="train.to_json_value" href="#train.to_json_value">to_json_value()</a></code> on an object.</p></div>
</dd>
<dt id="train.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>args: <a title="train.Args" href="#train.Args">Args</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="train.Args"><code class="flex name class">
<span>class <span class="ident">Args</span></span>
<span>(</span><span>batch_size: int = 128, n_epochs: int = 40, data_url: str = '/fs/ess/PAS2136/open_clip/data/evobio10m-v3.3/224x224/train/shard-{000000..000159}.tar', n_imgs: int = 9562377, n_workers: int = 4, resize_size: int = 256, crop_size: int = 224, tgt_mask_scale: tuple[float, float] = (0.15, 0.2), tgt_mask_aspect_ratio: tuple[float, float] = (0.75, 1.5), n_tgt_blocks: int = 4, ctx_mask_scale: tuple[float, float] = (0.85, 1.0), ctx_mask_aspect_ratio: tuple[float, float] = (1.0, 1.0), min_ctx_patches: int = 10, d_vit: int = 768, n_vit_heads: int = 12, n_vit_layers: int = 12, p_vit_dropout: float = 0.1, patch_size: int = 16, d_pred: int = 384, n_pred_heads: int = 12, n_pred_layers: int = 6, p_pred_dropout: float = 0.1, lr_init: float = 0.0001, lr_max: float = 0.001, lr_final: float = 1e-06, n_lr_warmup: int = 143435655, newt_args: typing.Annotated[<a title="newt.Args" href="newt.html#newt.Args">Args</a>, _ArgConfiguration(name='<a title="newt" href="newt.html">newt</a>', metavar=None, help=None, help_behavior_hint=None, aliases=None, prefix_name=None, constructor_factory=None)] = &lt;factory&gt;, eval_every: int = 10000, device: str = 'cuda', gpus_per_node: int = 4, cpus_per_task: int = 12, slurm: bool = True, slurm_acct: str = 'PAS2136', wandb_entity: str = 'samuelstevens', wandb_project: str = 'ijepa', track: bool = True, log_every: int = 10, log_to: str = 'logs', is_ddp: bool = False, local_rank: int = 0, global_rank: int = 0, is_master: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Configuration for a training run.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
@dataclasses.dataclass(frozen=True)
class Args:
    &#34;&#34;&#34;Configuration for a training run.&#34;&#34;&#34;

    batch_size: int = 128
    &#34;&#34;&#34;Training batch size (per device).&#34;&#34;&#34;
    n_epochs: int = 40
    &#34;&#34;&#34;Number of training epochs.&#34;&#34;&#34;

    # Data
    data_url: str = &#34;/fs/ess/PAS2136/open_clip/data/evobio10m-v3.3/224x224/train/shard-{000000..000159}.tar&#34;
    &#34;&#34;&#34;Path to webdataset shards.&#34;&#34;&#34;
    n_imgs: int = 9562377
    &#34;&#34;&#34;Number of images in the training data.&#34;&#34;&#34;
    n_workers: int = 4
    &#34;&#34;&#34;Number of workers to load data with.&#34;&#34;&#34;
    resize_size: int = 256
    &#34;&#34;&#34;How big to resize images.&#34;&#34;&#34;
    crop_size: int = 224
    &#34;&#34;&#34;After resize, how big an image to crop.&#34;&#34;&#34;

    # I-JEPA Objective
    tgt_mask_scale: tuple[float, float] = (0.15, 0.2)
    &#34;&#34;&#34;Target block mask size range.&#34;&#34;&#34;
    tgt_mask_aspect_ratio: tuple[float, float] = (0.75, 1.5)
    &#34;&#34;&#34;Target block mask aspect ratio range.&#34;&#34;&#34;
    n_tgt_blocks: int = 4
    &#34;&#34;&#34;Number of target blocks.&#34;&#34;&#34;
    ctx_mask_scale: tuple[float, float] = (0.85, 1.0)
    &#34;&#34;&#34;Context block mask size range.&#34;&#34;&#34;
    ctx_mask_aspect_ratio: tuple[float, float] = (1.0, 1.0)
    &#34;&#34;&#34;Context block mask aspect ratio range.&#34;&#34;&#34;
    min_ctx_patches: int = 10
    &#34;&#34;&#34;Minimum number of context patches.&#34;&#34;&#34;

    # Modeling
    d_vit: int = 768
    &#34;&#34;&#34;ViT&#39;s residual dimension.&#34;&#34;&#34;
    n_vit_heads: int = 12
    &#34;&#34;&#34;Number of attention heads for ViT.&#34;&#34;&#34;
    n_vit_layers: int = 12
    &#34;&#34;&#34;Number of ViT layers.&#34;&#34;&#34;
    p_vit_dropout: float = 0.1
    &#34;&#34;&#34;Probability of dropout in ViT.&#34;&#34;&#34;
    patch_size: int = 16
    &#34;&#34;&#34;Patch size for the ViT.&#34;&#34;&#34;

    d_pred: int = 384
    &#34;&#34;&#34;Predictor transformer&#39;s residual dimension.&#34;&#34;&#34;
    n_pred_heads: int = 12
    &#34;&#34;&#34;Number of predictor transformer attention heads (kept same as ViT).&#34;&#34;&#34;
    n_pred_layers: int = 6
    &#34;&#34;&#34;Number of predictor transformer layers.&#34;&#34;&#34;
    p_pred_dropout: float = 0.1
    &#34;&#34;&#34;Probability of dropout in predictor transformer.&#34;&#34;&#34;

    # Optimization
    lr_init: float = 1e-4
    &#34;&#34;&#34;Initial learning rate.&#34;&#34;&#34;
    lr_max: float = 1e-3
    &#34;&#34;&#34;Maximum learning rate.&#34;&#34;&#34;
    lr_final: float = 1e-6
    &#34;&#34;&#34;Final learning rate.&#34;&#34;&#34;
    n_lr_warmup: int = 143_435_655
    &#34;&#34;&#34;Number of learning rate warmup steps.&#34;&#34;&#34;
    wd_init = 0.04
    &#34;&#34;&#34;Initial weight decay.&#34;&#34;&#34;
    wd_final = 0.4
    &#34;&#34;&#34;Final weight decay.&#34;&#34;&#34;
    momentum_init = 0.996
    &#34;&#34;&#34;Initial momentum.&#34;&#34;&#34;
    momentum_final = 1.0
    &#34;&#34;&#34;Final momentum.&#34;&#34;&#34;

    # Evaluation
    # The typing.Annotated and tyro code makes the launch.py CLI args clean.
    newt_args: typing.Annotated[newt.Args, tyro.conf.arg(name=&#34;newt&#34;)] = (
        dataclasses.field(
            default_factory=lambda: newt.Args(
                data=&#34;/fs/scratch/PAS2136/samuelstevens/datasets/newt&#34;
            )
        )
    )
    eval_every: int = 10_000

    # Hardware
    device: str = &#34;cuda&#34;
    &#34;&#34;&#34;Hardware accelerator (if any); either &#39;cpu&#39; or &#39;cuda&#39;. Do not specify specific GPUs; use CUDA_VISIBLE_DEVICES for that.&#34;&#34;&#34;
    gpus_per_node: int = 4
    &#34;&#34;&#34;Number of GPUs per node.&#34;&#34;&#34;
    cpus_per_task: int = 12
    &#34;&#34;&#34;Number of CPUs per task.&#34;&#34;&#34;
    slurm: bool = True
    &#34;&#34;&#34;Whether to run on a slurm cluster.&#34;&#34;&#34;
    slurm_acct: str = &#34;PAS2136&#34;
    &#34;&#34;&#34;Slurm account.&#34;&#34;&#34;

    # Misc
    wandb_entity: str = &#34;samuelstevens&#34;
    &#34;&#34;&#34;WandB entity.&#34;&#34;&#34;
    wandb_project: str = &#34;ijepa&#34;
    &#34;&#34;&#34;WandB project.&#34;&#34;&#34;
    track: bool = True
    &#34;&#34;&#34;Whether to track runs with WandB.&#34;&#34;&#34;
    log_every: int = 10
    &#34;&#34;&#34;How often to log to WandB.&#34;&#34;&#34;
    log_to: str = &#34;logs&#34;
    &#34;&#34;&#34;Where to write logs.&#34;&#34;&#34;

    # Distributed (set at runtime)
    is_ddp: bool = False
    &#34;&#34;&#34;(set at runtime) Whether we are in a distributed setting.&#34;&#34;&#34;
    local_rank: int = 0
    &#34;&#34;&#34;(set at runtime) Local (current node) rank.&#34;&#34;&#34;
    global_rank: int = 0
    &#34;&#34;&#34;(set at runtime) Global (world) rank.&#34;&#34;&#34;
    is_master: bool = True
    &#34;&#34;&#34;(set at runtime) Whether this process is global master.&#34;&#34;&#34;

    @property
    def size_p(self) -&gt; tuple[int, int]:
        &#34;&#34;&#34;
        Size of inputs in patches.
        &#34;&#34;&#34;
        assert self.crop_size % self.patch_size == 0
        w_p = h_p = self.crop_size // self.patch_size
        return (w_p, h_p)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="train.Args.batch_size"><code class="name">var <span class="ident">batch_size</span> : int</code></dt>
<dd>
<div class="desc"><p>Training batch size (per device).</p></div>
</dd>
<dt id="train.Args.cpus_per_task"><code class="name">var <span class="ident">cpus_per_task</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of CPUs per task.</p></div>
</dd>
<dt id="train.Args.crop_size"><code class="name">var <span class="ident">crop_size</span> : int</code></dt>
<dd>
<div class="desc"><p>After resize, how big an image to crop.</p></div>
</dd>
<dt id="train.Args.ctx_mask_aspect_ratio"><code class="name">var <span class="ident">ctx_mask_aspect_ratio</span> : tuple[float, float]</code></dt>
<dd>
<div class="desc"><p>Context block mask aspect ratio range.</p></div>
</dd>
<dt id="train.Args.ctx_mask_scale"><code class="name">var <span class="ident">ctx_mask_scale</span> : tuple[float, float]</code></dt>
<dd>
<div class="desc"><p>Context block mask size range.</p></div>
</dd>
<dt id="train.Args.d_pred"><code class="name">var <span class="ident">d_pred</span> : int</code></dt>
<dd>
<div class="desc"><p>Predictor transformer's residual dimension.</p></div>
</dd>
<dt id="train.Args.d_vit"><code class="name">var <span class="ident">d_vit</span> : int</code></dt>
<dd>
<div class="desc"><p>ViT's residual dimension.</p></div>
</dd>
<dt id="train.Args.data_url"><code class="name">var <span class="ident">data_url</span> : str</code></dt>
<dd>
<div class="desc"><p>Path to webdataset shards.</p></div>
</dd>
<dt id="train.Args.device"><code class="name">var <span class="ident">device</span> : str</code></dt>
<dd>
<div class="desc"><p>Hardware accelerator (if any); either 'cpu' or 'cuda'. Do not specify specific GPUs; use CUDA_VISIBLE_DEVICES for that.</p></div>
</dd>
<dt id="train.Args.eval_every"><code class="name">var <span class="ident">eval_every</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="train.Args.global_rank"><code class="name">var <span class="ident">global_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>(set at runtime) Global (world) rank.</p></div>
</dd>
<dt id="train.Args.gpus_per_node"><code class="name">var <span class="ident">gpus_per_node</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of GPUs per node.</p></div>
</dd>
<dt id="train.Args.is_ddp"><code class="name">var <span class="ident">is_ddp</span> : bool</code></dt>
<dd>
<div class="desc"><p>(set at runtime) Whether we are in a distributed setting.</p></div>
</dd>
<dt id="train.Args.is_master"><code class="name">var <span class="ident">is_master</span> : bool</code></dt>
<dd>
<div class="desc"><p>(set at runtime) Whether this process is global master.</p></div>
</dd>
<dt id="train.Args.local_rank"><code class="name">var <span class="ident">local_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>(set at runtime) Local (current node) rank.</p></div>
</dd>
<dt id="train.Args.log_every"><code class="name">var <span class="ident">log_every</span> : int</code></dt>
<dd>
<div class="desc"><p>How often to log to WandB.</p></div>
</dd>
<dt id="train.Args.log_to"><code class="name">var <span class="ident">log_to</span> : str</code></dt>
<dd>
<div class="desc"><p>Where to write logs.</p></div>
</dd>
<dt id="train.Args.lr_final"><code class="name">var <span class="ident">lr_final</span> : float</code></dt>
<dd>
<div class="desc"><p>Final learning rate.</p></div>
</dd>
<dt id="train.Args.lr_init"><code class="name">var <span class="ident">lr_init</span> : float</code></dt>
<dd>
<div class="desc"><p>Initial learning rate.</p></div>
</dd>
<dt id="train.Args.lr_max"><code class="name">var <span class="ident">lr_max</span> : float</code></dt>
<dd>
<div class="desc"><p>Maximum learning rate.</p></div>
</dd>
<dt id="train.Args.min_ctx_patches"><code class="name">var <span class="ident">min_ctx_patches</span> : int</code></dt>
<dd>
<div class="desc"><p>Minimum number of context patches.</p></div>
</dd>
<dt id="train.Args.momentum_final"><code class="name">var <span class="ident">momentum_final</span></code></dt>
<dd>
<div class="desc"><p>Final momentum.</p></div>
</dd>
<dt id="train.Args.momentum_init"><code class="name">var <span class="ident">momentum_init</span></code></dt>
<dd>
<div class="desc"><p>Initial momentum.</p></div>
</dd>
<dt id="train.Args.n_epochs"><code class="name">var <span class="ident">n_epochs</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of training epochs.</p></div>
</dd>
<dt id="train.Args.n_imgs"><code class="name">var <span class="ident">n_imgs</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of images in the training data.</p></div>
</dd>
<dt id="train.Args.n_lr_warmup"><code class="name">var <span class="ident">n_lr_warmup</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of learning rate warmup steps.</p></div>
</dd>
<dt id="train.Args.n_pred_heads"><code class="name">var <span class="ident">n_pred_heads</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of predictor transformer attention heads (kept same as ViT).</p></div>
</dd>
<dt id="train.Args.n_pred_layers"><code class="name">var <span class="ident">n_pred_layers</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of predictor transformer layers.</p></div>
</dd>
<dt id="train.Args.n_tgt_blocks"><code class="name">var <span class="ident">n_tgt_blocks</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of target blocks.</p></div>
</dd>
<dt id="train.Args.n_vit_heads"><code class="name">var <span class="ident">n_vit_heads</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of attention heads for ViT.</p></div>
</dd>
<dt id="train.Args.n_vit_layers"><code class="name">var <span class="ident">n_vit_layers</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of ViT layers.</p></div>
</dd>
<dt id="train.Args.n_workers"><code class="name">var <span class="ident">n_workers</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of workers to load data with.</p></div>
</dd>
<dt id="train.Args.newt_args"><code class="name">var <span class="ident">newt_args</span> : <a title="newt.Args" href="newt.html#newt.Args">Args</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="train.Args.p_pred_dropout"><code class="name">var <span class="ident">p_pred_dropout</span> : float</code></dt>
<dd>
<div class="desc"><p>Probability of dropout in predictor transformer.</p></div>
</dd>
<dt id="train.Args.p_vit_dropout"><code class="name">var <span class="ident">p_vit_dropout</span> : float</code></dt>
<dd>
<div class="desc"><p>Probability of dropout in ViT.</p></div>
</dd>
<dt id="train.Args.patch_size"><code class="name">var <span class="ident">patch_size</span> : int</code></dt>
<dd>
<div class="desc"><p>Patch size for the ViT.</p></div>
</dd>
<dt id="train.Args.resize_size"><code class="name">var <span class="ident">resize_size</span> : int</code></dt>
<dd>
<div class="desc"><p>How big to resize images.</p></div>
</dd>
<dt id="train.Args.slurm"><code class="name">var <span class="ident">slurm</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether to run on a slurm cluster.</p></div>
</dd>
<dt id="train.Args.slurm_acct"><code class="name">var <span class="ident">slurm_acct</span> : str</code></dt>
<dd>
<div class="desc"><p>Slurm account.</p></div>
</dd>
<dt id="train.Args.tgt_mask_aspect_ratio"><code class="name">var <span class="ident">tgt_mask_aspect_ratio</span> : tuple[float, float]</code></dt>
<dd>
<div class="desc"><p>Target block mask aspect ratio range.</p></div>
</dd>
<dt id="train.Args.tgt_mask_scale"><code class="name">var <span class="ident">tgt_mask_scale</span> : tuple[float, float]</code></dt>
<dd>
<div class="desc"><p>Target block mask size range.</p></div>
</dd>
<dt id="train.Args.track"><code class="name">var <span class="ident">track</span> : bool</code></dt>
<dd>
<div class="desc"><p>Whether to track runs with WandB.</p></div>
</dd>
<dt id="train.Args.wandb_entity"><code class="name">var <span class="ident">wandb_entity</span> : str</code></dt>
<dd>
<div class="desc"><p>WandB entity.</p></div>
</dd>
<dt id="train.Args.wandb_project"><code class="name">var <span class="ident">wandb_project</span> : str</code></dt>
<dd>
<div class="desc"><p>WandB project.</p></div>
</dd>
<dt id="train.Args.wd_final"><code class="name">var <span class="ident">wd_final</span></code></dt>
<dd>
<div class="desc"><p>Final weight decay.</p></div>
</dd>
<dt id="train.Args.wd_init"><code class="name">var <span class="ident">wd_init</span></code></dt>
<dd>
<div class="desc"><p>Initial weight decay.</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="train.Args.size_p"><code class="name">prop <span class="ident">size_p</span> : tuple[int, int]</code></dt>
<dd>
<div class="desc"><p>Size of inputs in patches.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size_p(self) -&gt; tuple[int, int]:
    &#34;&#34;&#34;
    Size of inputs in patches.
    &#34;&#34;&#34;
    assert self.crop_size % self.patch_size == 0
    w_p = h_p = self.crop_size // self.patch_size
    return (w_p, h_p)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="train.MaskCollator"><code class="flex name class">
<span>class <span class="ident">MaskCollator</span></span>
<span>(</span><span>args: <a title="train.Args" href="#train.Args">Args</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>MaskCollator chooses masks for a given batch.</p>
<p>I-JEPA makes use of several masks.</p>
<p>First, the context mask is the set of patches that the context encoder embeds. These masks are referred to with the <code>ctx_</code> prefix.</p>
<p>Second, the target masks is the set of patch sets that the narrow predictor network tries to predict. These masks and patches are referred to with the <code>tgt_</code> prefix.</p>
<p>The context mask cannot overlap with the target mask, but the context mask must have a minimum number of patches (<code><a title="train.Args.min_ctx_patches" href="#train.Args.min_ctx_patches">Args.min_ctx_patches</a></code>).</p>
<p>TODO: make it not int64 because it's very inefficient for GPUs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@beartype.beartype
class MaskCollator:
    &#34;&#34;&#34;
    MaskCollator chooses masks for a given batch.

    I-JEPA makes use of several masks.

    First, the context mask is the set of patches that the context encoder embeds. These masks are referred to with the `ctx_` prefix.

    Second, the target masks is the set of patch sets that the narrow predictor network tries to predict. These masks and patches are referred to with the `tgt_` prefix.

    The context mask cannot overlap with the target mask, but the context mask must have a minimum number of patches (`Args.min_ctx_patches`).

    TODO: make it not int64 because it&#39;s very inefficient for GPUs.
    &#34;&#34;&#34;

    def __init__(self, args: Args):
        self.tgt_mask_scale = args.tgt_mask_scale
        self.tgt_mask_aspect_ratio = args.tgt_mask_aspect_ratio
        self.n_tgt_blocks = args.n_tgt_blocks

        self.ctx_mask_scale = args.ctx_mask_scale
        self.ctx_mask_aspect_ratio = args.ctx_mask_aspect_ratio
        self.min_ctx_patches = args.min_ctx_patches

        assert args.crop_size % args.patch_size == 0
        self.size = (
            args.crop_size // args.patch_size,
            args.crop_size // args.patch_size,
        )

    def sample_block_size(
        self,
        scale: tuple[float, float],
        aspect_ratio: tuple[float, float],
        size: tuple[int, int],
    ) -&gt; tuple[int, int]:
        &#34;&#34;&#34;
        Sample a (width&#39;, height&#39;) that is within the scale range and the aspect ratio range with respect to the original size (width, height).

        Note that once we know the sampled scale and aspect ratio, then:

        w&#39; * h&#39; = w * h * scale

        w&#39; / h&#39; = aspect

        Then with algebra we can find the expresions for h&#39; and w&#39;.
        &#34;&#34;&#34;
        r = torch.rand(1).item()
        min_s, max_s = scale
        mask_scale = min_s + r * (max_s - min_s)

        min_a, max_a = aspect_ratio
        mask_aspect_ratio = min_a + r * (max_a - min_a)

        w, h = size

        h_ = math.sqrt(w * h * mask_scale / mask_aspect_ratio)
        w_ = int(round(mask_aspect_ratio * h_))
        h_ = int(round(h_))

        # Can&#39;t be bigger than original size.
        h_ = min(h_, h)
        w_ = min(w_, w)

        return w_, h_

    @jaxtyped(typechecker=beartype.beartype)
    def sample_block_mask(
        self,
        block_size_p: tuple[int, int],
        size_p: tuple[int, int],
        *,
        unacceptable: list[Int[Tensor, &#34;w_patch h_patch&#34;]],
    ) -&gt; Int[Tensor, &#34;w_patch h_patch&#34;]:
        &#34;&#34;&#34;
        Sample a block mask. When picking the context mask, we have to ignore the target masks. If it&#39;s impossible to sample a context mask that satisfies the overlap constraints, we will allow some overlap between target masks and the context masks.

        Args:
            block_size_p: block width and height in patches.
        &#34;&#34;&#34;

        def constrain_mask(mask: Int[Tensor, &#34;width height&#34;]):
            for complement in unacceptable:
                mask *= 1 - complement

        b_wp, b_hp = block_size_p
        wp, hp = size_p
        valid = False
        while not valid:
            top = torch.randint(0, hp - b_hp + 1, (1,))
            left = torch.randint(0, wp - b_wp + 1, (1,))
            mask = torch.zeros(size_p, dtype=int)
            mask[left : left + b_wp, top : top + b_hp] = 1
            if unacceptable:
                constrain_mask(mask)
            valid = mask.sum().item() &gt;= self.min_ctx_patches

            if not valid:
                print(&#34;invalid&#34;)

        return mask

    @jaxtyped(typechecker=beartype.beartype)
    def __call__(
        self, batch: list[Float[Tensor, &#34;batch 3 w_img h_img&#34;]]
    ) -&gt; tuple[
        Float[Tensor, &#34;batch 3 w_img h_img&#34;],
        list[Int[Tensor, &#34; n_tgt_patch&#34;]],
        Int[Tensor, &#34; n_ctx_patch&#34;],
    ]:
        &#34;&#34;&#34; &#34;&#34;&#34;
        assert len(batch) == 1, f&#34;len(batch) == {len(batch)} != 1.&#34;
        batch = batch[0]

        tgt_masks = []

        for _ in range(self.n_tgt_blocks):
            block_size = self.sample_block_size(
                self.tgt_mask_scale, self.tgt_mask_aspect_ratio, self.size
            )
            tgt_masks.append(
                self.sample_block_mask(block_size, self.size, unacceptable=[])
            )

        block_size = self.sample_block_size(
            self.ctx_mask_scale, self.ctx_mask_aspect_ratio, self.size
        )
        ctx_mask = self.sample_block_mask(block_size, self.size, unacceptable=tgt_masks)

        ctx_mask = einops.rearrange(ctx_mask, &#34;wp hp -&gt; (wp hp)&#34;)
        ctx_mask = ctx_mask.nonzero().squeeze()

        tgt_masks = torch.stack(tgt_masks)
        tgt_masks = einops.rearrange(tgt_masks, &#34;n wp hp -&gt; n (wp hp)&#34;)
        tgt_masks = [mask.nonzero().squeeze() for mask in tgt_masks]

        return batch, tgt_masks, ctx_mask</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="train.MaskCollator.sample_block_mask"><code class="name flex">
<span>def <span class="ident">sample_block_mask</span></span>(<span>self, block_size_p: tuple[int, int], size_p: tuple[int, int], *, unacceptable: list[jaxtyping.Int[Tensor, 'w_patch h_patch']]) ‑> jaxtyping.Int[Tensor, 'w_patch h_patch']</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a block mask. When picking the context mask, we have to ignore the target masks. If it's impossible to sample a context mask that satisfies the overlap constraints, we will allow some overlap between target masks and the context masks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>block_size_p</code></strong></dt>
<dd>block width and height in patches.</dd>
</dl></div>
</dd>
<dt id="train.MaskCollator.sample_block_size"><code class="name flex">
<span>def <span class="ident">sample_block_size</span></span>(<span>self, scale: tuple[float, float], aspect_ratio: tuple[float, float], size: tuple[int, int]) ‑> tuple[int, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a (width', height') that is within the scale range and the aspect ratio range with respect to the original size (width, height).</p>
<p>Note that once we know the sampled scale and aspect ratio, then:</p>
<p>w' * h' = w * h * scale</p>
<p>w' / h' = aspect</p>
<p>Then with algebra we can find the expresions for h' and w'.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="train.filter_no_caption_or_no_image" href="#train.filter_no_caption_or_no_image">filter_no_caption_or_no_image</a></code></li>
<li><code><a title="train.log_and_continue" href="#train.log_and_continue">log_and_continue</a></code></li>
<li><code><a title="train.make_dataloader" href="#train.make_dataloader">make_dataloader</a></code></li>
<li><code><a title="train.make_img_transform" href="#train.make_img_transform">make_img_transform</a></code></li>
<li><code><a title="train.print_env" href="#train.print_env">print_env</a></code></li>
<li><code><a title="train.to_json_value" href="#train.to_json_value">to_json_value</a></code></li>
<li><code><a title="train.train" href="#train.train">train</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="train.Args" href="#train.Args">Args</a></code></h4>
<ul class="">
<li><code><a title="train.Args.batch_size" href="#train.Args.batch_size">batch_size</a></code></li>
<li><code><a title="train.Args.cpus_per_task" href="#train.Args.cpus_per_task">cpus_per_task</a></code></li>
<li><code><a title="train.Args.crop_size" href="#train.Args.crop_size">crop_size</a></code></li>
<li><code><a title="train.Args.ctx_mask_aspect_ratio" href="#train.Args.ctx_mask_aspect_ratio">ctx_mask_aspect_ratio</a></code></li>
<li><code><a title="train.Args.ctx_mask_scale" href="#train.Args.ctx_mask_scale">ctx_mask_scale</a></code></li>
<li><code><a title="train.Args.d_pred" href="#train.Args.d_pred">d_pred</a></code></li>
<li><code><a title="train.Args.d_vit" href="#train.Args.d_vit">d_vit</a></code></li>
<li><code><a title="train.Args.data_url" href="#train.Args.data_url">data_url</a></code></li>
<li><code><a title="train.Args.device" href="#train.Args.device">device</a></code></li>
<li><code><a title="train.Args.eval_every" href="#train.Args.eval_every">eval_every</a></code></li>
<li><code><a title="train.Args.global_rank" href="#train.Args.global_rank">global_rank</a></code></li>
<li><code><a title="train.Args.gpus_per_node" href="#train.Args.gpus_per_node">gpus_per_node</a></code></li>
<li><code><a title="train.Args.is_ddp" href="#train.Args.is_ddp">is_ddp</a></code></li>
<li><code><a title="train.Args.is_master" href="#train.Args.is_master">is_master</a></code></li>
<li><code><a title="train.Args.local_rank" href="#train.Args.local_rank">local_rank</a></code></li>
<li><code><a title="train.Args.log_every" href="#train.Args.log_every">log_every</a></code></li>
<li><code><a title="train.Args.log_to" href="#train.Args.log_to">log_to</a></code></li>
<li><code><a title="train.Args.lr_final" href="#train.Args.lr_final">lr_final</a></code></li>
<li><code><a title="train.Args.lr_init" href="#train.Args.lr_init">lr_init</a></code></li>
<li><code><a title="train.Args.lr_max" href="#train.Args.lr_max">lr_max</a></code></li>
<li><code><a title="train.Args.min_ctx_patches" href="#train.Args.min_ctx_patches">min_ctx_patches</a></code></li>
<li><code><a title="train.Args.momentum_final" href="#train.Args.momentum_final">momentum_final</a></code></li>
<li><code><a title="train.Args.momentum_init" href="#train.Args.momentum_init">momentum_init</a></code></li>
<li><code><a title="train.Args.n_epochs" href="#train.Args.n_epochs">n_epochs</a></code></li>
<li><code><a title="train.Args.n_imgs" href="#train.Args.n_imgs">n_imgs</a></code></li>
<li><code><a title="train.Args.n_lr_warmup" href="#train.Args.n_lr_warmup">n_lr_warmup</a></code></li>
<li><code><a title="train.Args.n_pred_heads" href="#train.Args.n_pred_heads">n_pred_heads</a></code></li>
<li><code><a title="train.Args.n_pred_layers" href="#train.Args.n_pred_layers">n_pred_layers</a></code></li>
<li><code><a title="train.Args.n_tgt_blocks" href="#train.Args.n_tgt_blocks">n_tgt_blocks</a></code></li>
<li><code><a title="train.Args.n_vit_heads" href="#train.Args.n_vit_heads">n_vit_heads</a></code></li>
<li><code><a title="train.Args.n_vit_layers" href="#train.Args.n_vit_layers">n_vit_layers</a></code></li>
<li><code><a title="train.Args.n_workers" href="#train.Args.n_workers">n_workers</a></code></li>
<li><code><a title="train.Args.newt_args" href="#train.Args.newt_args">newt_args</a></code></li>
<li><code><a title="train.Args.p_pred_dropout" href="#train.Args.p_pred_dropout">p_pred_dropout</a></code></li>
<li><code><a title="train.Args.p_vit_dropout" href="#train.Args.p_vit_dropout">p_vit_dropout</a></code></li>
<li><code><a title="train.Args.patch_size" href="#train.Args.patch_size">patch_size</a></code></li>
<li><code><a title="train.Args.resize_size" href="#train.Args.resize_size">resize_size</a></code></li>
<li><code><a title="train.Args.size_p" href="#train.Args.size_p">size_p</a></code></li>
<li><code><a title="train.Args.slurm" href="#train.Args.slurm">slurm</a></code></li>
<li><code><a title="train.Args.slurm_acct" href="#train.Args.slurm_acct">slurm_acct</a></code></li>
<li><code><a title="train.Args.tgt_mask_aspect_ratio" href="#train.Args.tgt_mask_aspect_ratio">tgt_mask_aspect_ratio</a></code></li>
<li><code><a title="train.Args.tgt_mask_scale" href="#train.Args.tgt_mask_scale">tgt_mask_scale</a></code></li>
<li><code><a title="train.Args.track" href="#train.Args.track">track</a></code></li>
<li><code><a title="train.Args.wandb_entity" href="#train.Args.wandb_entity">wandb_entity</a></code></li>
<li><code><a title="train.Args.wandb_project" href="#train.Args.wandb_project">wandb_project</a></code></li>
<li><code><a title="train.Args.wd_final" href="#train.Args.wd_final">wd_final</a></code></li>
<li><code><a title="train.Args.wd_init" href="#train.Args.wd_init">wd_init</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="train.MaskCollator" href="#train.MaskCollator">MaskCollator</a></code></h4>
<ul class="">
<li><code><a title="train.MaskCollator.sample_block_mask" href="#train.MaskCollator.sample_block_mask">sample_block_mask</a></code></li>
<li><code><a title="train.MaskCollator.sample_block_size" href="#train.MaskCollator.sample_block_size">sample_block_size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.1</a>.</p>
</footer>
</body>
</html>
